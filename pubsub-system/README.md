# In-Memory Pub/Sub System

A production-grade, in-memory Pub/Sub system built with FastAPI and WebSockets. This system provides robust message delivery with proper concurrency control, backpressure handling, and graceful shutdown.

## üèóÔ∏è Architecture Overview

### Core Components

```
src/
‚îú‚îÄ‚îÄ main.py              # Application bootstrap & REST API
‚îú‚îÄ‚îÄ ws/                  # WebSocket protocol handling
‚îÇ   ‚îî‚îÄ‚îÄ handler.py       # Message routing & connection management
‚îú‚îÄ‚îÄ topics/              # Topic & subscription logic
‚îÇ   ‚îú‚îÄ‚îÄ topic_manager.py # Topic lifecycle & delivery workers
‚îÇ   ‚îî‚îÄ‚îÄ subscriber.py    # Subscriber connection management
‚îú‚îÄ‚îÄ models/              # Message schemas & API models
‚îÇ   ‚îú‚îÄ‚îÄ messages.py      # WebSocket message types
‚îÇ   ‚îî‚îÄ‚îÄ api.py          # REST API models
‚îî‚îÄ‚îÄ utils/               # Helper utilities
    ‚îú‚îÄ‚îÄ ring_buffer.py   # Thread-safe ring buffer for replay
    ‚îú‚îÄ‚îÄ time_utils.py    # Timestamp utilities
    ‚îî‚îÄ‚îÄ validation.py    # Input validation
```

### Design Principles

1. **Correctness over speed**: All operations are thread-safe and race-condition free
2. **No global locks during publishing**: Each topic has its own lock
3. **Bounded memory**: All queues have maximum sizes
4. **Predictable failure modes**: Slow consumers don't block publishers
5. **Clean separation**: REST and WebSocket layers are decoupled
6. **Low latency**: Topic-level delivery workers with batching reduce overhead

### Topic-Level Delivery Architecture

**ARCHITECTURE CHANGE (Latency Optimization):**

Instead of per-subscriber queues and delivery loops, the system now uses:

- **Topic-level delivery workers**: Each topic has one async worker
- **Message batching**: Workers accumulate messages (batch size: 10, timeout: 20ms)
- **Concurrent fan-out**: Batches are delivered to all subscribers in parallel
- **No per-subscriber queues**: Eliminates queue buildup and reduces latency

**Benefits:**
- ‚úÖ **Lower latency**: Messages delivered within 20ms instead of accumulating in queues
- ‚úÖ **Better throughput**: Batching reduces per-message overhead
- ‚úÖ **Concurrent delivery**: All subscribers receive messages in parallel
- ‚úÖ **Preserved ordering**: Messages delivered in publish order within each topic

**Tradeoff:**
- ‚ö†Ô∏è **Completeness vs latency**: Slow subscribers may miss messages if they can't keep up
- This is acceptable for real-time systems where fresh data matters more than completeness

## üîí Concurrency Strategy

### Per-Topic Locking

- **Global lock**: Only used for topic creation/deletion
- **Topic-level locks**: Each topic has its own lock for subscriber operations
- **No blocking**: Publishers never wait for slow consumers

### Thread Safety Guarantees

- ‚úÖ Concurrent subscriptions to different topics
- ‚úÖ Concurrent publishing to different topics
- ‚úÖ Safe topic deletion with active subscribers
- ‚úÖ Safe subscriber cleanup on disconnect

## üö¶ Backpressure Policy

**Policy: Disconnect Slow Consumers**

With topic-level delivery workers, backpressure is handled differently:

- **Batch delivery**: Messages are sent in batches to all subscribers concurrently
- **Slow consumer detection**: If a subscriber fails to receive a batch, it's marked as closed
- **Automatic removal**: Failed subscribers are removed from the topic
- **Publishers never blocked**: Publishing is async and non-blocking

### Why This Policy?

1. **Publisher protection**: Publishers never wait for slow consumers
2. **Memory bounds**: Topic queues are bounded (10,000 messages max)
3. **System stability**: One slow consumer cannot affect others
4. **Low latency**: No per-subscriber queue buildup

### Latency vs Completeness Tradeoff

**Previous architecture**: Per-subscriber queues with drop-oldest
- ‚úÖ High completeness (messages queued until delivered)
- ‚ùå High latency (queue buildup under load)

**Current architecture**: Topic-level batching with concurrent delivery
- ‚úÖ Low latency (messages delivered within 20ms)
- ‚ö†Ô∏è Lower completeness (slow subscribers may be disconnected)

This tradeoff is acceptable for real-time systems where **fresh data matters more than completeness**.

## ÔøΩ Security Features

### Rate Limiting

Per-connection publish rate limiting prevents abuse:

- **Algorithm**: Token bucket
- **Rate**: 1000 requests/second per connection
- **Burst**: 500 requests (handles legitimate bursts)
- **Response**: Returns `RATE_LIMITED` error with retry time

```json
{
  "type": "error",
  "code": "RATE_LIMITED",
  "message": "Publish rate limit exceeded. Try again in 0.50s",
  "details": {"retry_after_seconds": 0.5}
}
```

### Input Validation

- **Message size**: Max 64KB per message
- **Topic names**: Alphanumeric, hyphens, underscores (max 256 chars)
- **Replay limit**: Max 1000 messages for `last_n`

### Slow Consumer Detection

- **Timeout**: 500ms per batch send
- **Action**: Slow subscribers are disconnected
- **Benefit**: Prevents one slow client from affecting others

## ÔøΩüîÅ Message Replay Semantics

### Ring Buffer Per Topic

- Each topic maintains a ring buffer (default: 100 messages)
- Buffer size is configurable via `TopicManager(replay_buffer_size=N)`
- Thread-safe with internal locking

### Replay on Subscribe

When subscribing with `last_n > 0`:

1. **Replay happens first**: Historical messages are sent immediately
2. **Then live messages**: New messages are delivered after replay completes
3. **Non-blocking**: Replay doesn't block live publishing to other subscribers
4. **Bounded**: Maximum `last_n` is capped at 1000

### Example

```json
{
  "type": "subscribe",
  "topic": "events",
  "last_n": 10
}
```

This will:
1. Send the last 10 messages from the ring buffer
2. Then start delivering new messages as they arrive

## üì° WebSocket Protocol

### Client ‚Üí Server Messages

#### Subscribe
```json
{
  "type": "subscribe",
  "topic": "my-topic",
  "last_n": 10
}
```

#### Unsubscribe
```json
{
  "type": "unsubscribe",
  "topic": "my-topic"
}
```

#### Publish
```json
{
  "type": "publish",
  "topic": "my-topic",
  "data": {"any": "json", "data": 123}
}
```

#### Ping
```json
{
  "type": "ping"
}
```

### Server ‚Üí Client Messages

#### Ack
```json
{
  "type": "ack",
  "request_type": "subscribe",
  "topic": "my-topic",
  "message": "Subscribed to topic 'my-topic'"
}
```

#### Event
```json
{
  "type": "event",
  "topic": "my-topic",
  "data": {"any": "json"},
  "message_id": "uuid-here"
}
```

#### Error
```json
{
  "type": "error",
  "code": "TOPIC_NOT_FOUND",
  "message": "Topic 'my-topic' does not exist",
  "details": {}
}
```

#### Pong
```json
{
  "type": "pong"
}
```

#### Info
```json
{
  "type": "info",
  "message": "Connected with client_id: uuid-here"
}
```

## üåê REST API

### Create Topic
```bash
POST /topics
Content-Type: application/json

{
  "name": "my-topic"
}

Response: 201 Created
{
  "name": "my-topic",
  "created": true
}
```

**Idempotent**: Returns success even if topic exists.

### Delete Topic
```bash
DELETE /topics/{name}

Response: 204 No Content
```

Notifies all subscribers and cleans up resources.

### List Topics
```bash
GET /topics

Response: 200 OK
["topic1", "topic2", "topic3"]
```

### Health Check
```bash
GET /health

Response: 200 OK
{
  "status": "healthy",
  "uptime_seconds": 123.45,
  "topic_count": 5,
  "active_subscriber_count": 42
}
```

### Statistics
```bash
GET /stats

Response: 200 OK
{
  "topics": {
    "my-topic": {
      "message_count": 1000,
      "subscriber_count": 5
    }
  }
}
```

## üõë Graceful Shutdown

On receiving `SIGTERM` or `SIGINT`:

1. **Stop accepting new connections**: WebSocket endpoint returns 1001
2. **Stop accepting REST operations**: Returns 503 Service Unavailable
3. **Flush subscriber queues**: Best-effort delivery of pending messages
4. **Close all WebSockets**: Clean closure with proper cleanup
5. **Exit cleanly**: No deadlocks or crashes

### Testing Graceful Shutdown

```bash
# Start the server
python -m src.main

# In another terminal, send SIGTERM
kill -TERM <pid>

# Or use Ctrl+C for SIGINT
```

## üöÄ Running Locally

### Prerequisites

- Python 3.11+
- pip

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Run the server
python -m src.main
```

The server will start on `http://localhost:8000`

### API Documentation

Once running, visit:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## üê≥ Running with Docker

### Build the Image

```bash
docker build -t pubsub-system .
```

### Run the Container

```bash
docker run -p 8000:8000 pubsub-system
```

The server will be available at `http://localhost:8000`

## üìä Observability Dashboard (Optional)

A Streamlit-based read-only dashboard is provided to visualize internal system metrics such as queue depth, batching behavior, throughput, and latency. This dashboard does not participate in message delivery and is intended solely for monitoring and demonstration.

### Features

- **System Overview**: Active topics, subscribers, publish/delivery rates, uptime
- **Topic Drilldown**: Queue depth, batch size, message counts per topic
- **Latency Visualization**: Avg/P95/P99 latency with rolling window charts
- **Backpressure Visibility**: Drop counts, queue saturation indicators

### Running the Dashboard

```bash
# Install dashboard dependencies
pip install -r dashboard/requirements.txt

# Start the dashboard (backend must be running)
streamlit run dashboard/app.py
```

The dashboard will be available at `http://localhost:8501`

### Important Notes

- ‚ùå **NO publishing** messages from the dashboard
- ‚ùå **NO subscribing** clients from the dashboard
- ‚ùå **NO WebSocket** connections from the dashboard
- ‚ùå **NO control-plane** actions
- ‚úÖ **Read-only** metrics polling via HTTP

The dashboard polls the `/metrics` endpoint every second and maintains a rolling window of the last 120 samples for time-series visualization.

## üß™ Testing with WebSocket Client

### Using Python

```python
import asyncio
import websockets
import json

async def test_pubsub():
    uri = "ws://localhost:8000/ws"
    async with websockets.connect(uri) as websocket:
        # Subscribe to a topic
        await websocket.send(json.dumps({
            "type": "subscribe",
            "topic": "test-topic",
            "last_n": 5
        }))
        
        response = await websocket.recv()
        print(f"Received: {response}")
        
        # Publish a message
        await websocket.send(json.dumps({
            "type": "publish",
            "topic": "test-topic",
            "data": {"hello": "world"}
        }))
        
        # Receive the event
        event = await websocket.recv()
        print(f"Event: {event}")

asyncio.run(test_pubsub())
```

### Using wscat

```bash
# Install wscat
npm install -g wscat

# Connect
wscat -c ws://localhost:8000/ws

# Send messages
> {"type": "subscribe", "topic": "test", "last_n": 0}
> {"type": "publish", "topic": "test", "data": {"msg": "hello"}}
> {"type": "ping"}
```

## üìä Performance Characteristics

### Scalability

- **Topics**: Thousands of topics with minimal overhead
- **Subscribers**: Hundreds per topic without performance degradation
- **Messages**: High throughput with non-blocking publishers

### Memory Usage

- **Per topic**: ~1KB + (replay_buffer_size √ó avg_message_size) + queue overhead
- **Topic queue**: 500 messages max (configurable)
- **Replay buffer**: 100 messages per topic (configurable)
- **Bounded**: Maximum memory is predictable and configurable

### Latency (Measured)

- **Subscribe/Unsubscribe**: < 1ms
- **Publish**: < 1ms (non-blocking enqueue)
- **End-to-end delivery**: ~5-8ms average, ~20ms P99
- **Batch timeout**: 20ms max wait before flush

### Throughput (Measured)

- **Single topic**: ~1500 msg/s with 50 subscribers
- **High fanout**: ~630 msg/s with 10 subscribers, 5 publishers
- **Concurrent operations**: 50 topics created/deleted in < 0.1s

## ‚ö†Ô∏è Known Limitations

### 1. In-Memory Only

- **No persistence**: All data is lost on restart
- **No clustering**: Single-node only
- **Memory bound**: Limited by available RAM

### 2. Message Ordering

- **Per-topic ordering**: Guaranteed within a topic
- **Cross-topic ordering**: Not guaranteed
- **Replay ordering**: Matches publish order

### 3. Delivery Guarantees

- **At-most-once**: Messages may be dropped if subscriber queue is full
- **No acknowledgments**: No confirmation of message receipt
- **Best-effort**: Network failures may cause message loss

### 4. Scalability Limits

- **Single process**: No horizontal scaling
- **Thread-based**: Limited by Python GIL for CPU-bound operations
- **Network I/O**: Async I/O provides good concurrency for I/O-bound workloads

## üîß Configuration

### Environment Variables

```bash
# Server host (default: 0.0.0.0)
HOST=0.0.0.0

# Server port (default: 8000)
PORT=8000

# Log level (default: info)
LOG_LEVEL=info
```

### Code Configuration

Edit `src/main.py`:

```python
# Replay buffer size per topic
topic_manager = TopicManager(replay_buffer_size=100)
```

Edit `src/topics/topic_manager.py` for delivery tuning:

```python
# Topic queue size (lower = less latency, more drops)
TOPIC_QUEUE_MAX_SIZE = 500

# Batch settings
DEFAULT_BATCH_SIZE = 10        # Messages per batch
DEFAULT_BATCH_TIMEOUT_MS = 20  # Max wait before flush
DEFAULT_SEND_TIMEOUT_MS = 500  # Slow subscriber threshold
```

Edit `src/ws/handler.py` for rate limiting:

```python
RATE_LIMIT_REQUESTS_PER_SECOND = 1000
RATE_LIMIT_BURST = 500
MAX_MESSAGE_SIZE_BYTES = 65536  # 64KB
```

## üêõ Error Handling

### Client Errors (4xx)

- `400 Bad Request`: Invalid topic name or malformed request
- `404 Not Found`: Topic doesn't exist
- `503 Service Unavailable`: Server is shutting down

### Server Errors (5xx)

- `500 Internal Server Error`: Unexpected error (logged with traceback)

### WebSocket Errors

All errors are sent as structured JSON:

```json
{
  "type": "error",
  "code": "ERROR_CODE",
  "message": "Human-readable message",
  "details": {}
}
```

### Error Codes

- `INVALID_JSON`: Message is not valid JSON
- `INVALID_MESSAGE`: Message missing required fields
- `UNKNOWN_MESSAGE_TYPE`: Unsupported message type
- `VALIDATION_ERROR`: Message failed validation
- `TOPIC_NOT_FOUND`: Topic doesn't exist
- `NOT_SUBSCRIBED`: Not subscribed to topic
- `INTERNAL`: Internal server error

## üìù Logging

Logs are written to stdout in the format:

```
2024-01-30 09:45:00,123 - module_name - INFO - Log message
```

### Log Levels

- `INFO`: Normal operations (connections, subscriptions, etc.)
- `WARNING`: Recoverable issues (queue full, send failures)
- `ERROR`: Errors that don't crash the server
- `CRITICAL`: Fatal errors

## üéØ Production Considerations

### What This System Provides

‚úÖ Thread-safe operations  
‚úÖ Bounded memory usage  
‚úÖ Graceful shutdown  
‚úÖ Structured error handling  
‚úÖ Comprehensive logging  
‚úÖ Clean code architecture  

### What You'd Need for Production

- **Persistence**: Add Redis/PostgreSQL for message durability
- **Clustering**: Add message broker (NATS, RabbitMQ) for multi-node
- **Monitoring**: Add Prometheus metrics and health checks
- **Authentication**: Add JWT or API key authentication
- **Rate limiting**: Add per-client rate limits
- **Message TTL**: Add expiration for old messages

## üß™ Testing

The system includes comprehensive test suites to validate correctness and performance under real-world conditions.

### Test Suites

#### 1. Stress Tests (`tests/test_stress.py`)

Validates system correctness under various challenging scenarios:

- **Concurrent Subscribers**: 50-100 subscribers receiving messages simultaneously
- **Backpressure Handling**: Fast publisher with slow consumer (validates drop-oldest policy)
- **Message Replay**: Validates `last_n` parameter and replay ordering
- **Topic Deletion**: Ensures clean cleanup with active subscribers
- **Concurrent Operations**: Thread-safe topic creation/deletion
- **High Throughput**: Multiple publishers and subscribers on single topic
- **Reconnection**: WebSocket disconnect and reconnect handling

#### 2. Load Tests (`tests/load_test.py`)

Performance benchmarking with realistic production scenarios:

**Scenarios:**
- **Light Load**: 5 topics, 10 publishers, 20 subscribers (quick validation)
- **Medium Load**: 10 topics, 20 publishers, 50 subscribers (realistic production)
- **Heavy Load**: 20 topics, 50 publishers, 100 subscribers (stress test)
- **Extreme Load**: 50 topics, 100 publishers, 200 subscribers (breaking point)
- **Custom**: Configure your own parameters

**Metrics Tracked:**
- Message throughput (msg/s)
- End-to-end latency (avg, p50, p95, p99, max)
- Message delivery rate
- Dropped messages
- Error rates
- System health during load

### Running Tests

#### Quick Start

```bash
# Make test runner executable
chmod +x run_tests.sh

# Run tests (server must be running)
./run_tests.sh
```

#### Manual Execution

```bash
# Start the server in one terminal
./run.sh

# In another terminal, activate venv and run tests
source venv/bin/activate

# Run stress tests
python tests/test_stress.py

# Run load tests
python tests/load_test.py
```

### Expected Results

**Stress Tests:**
- All 7 tests should pass
- Validates correctness and safety guarantees

**Load Tests (Medium Load):**
- Throughput: 5,000-10,000 msg/s
- P99 Latency: < 100ms
- Message delivery: > 95%
- Error rate: < 1%

### Test Scenarios Explained

#### Backpressure Test
Publishes 2000 messages rapidly while subscriber processes slowly (50ms/msg). Validates:
- Publisher completes quickly (< 5s)
- Messages are dropped (confirming drop-oldest policy)
- System remains stable

#### Concurrent Subscribers Test
100 subscribers receive 50 messages each. Validates:
- All subscribers receive all messages
- Message ordering is preserved
- No cross-subscriber interference

#### High Throughput Test
5 publishers √ó 200 messages ‚Üí 10 subscribers. Validates:
- System handles concurrent load
- Latency remains acceptable
- No message loss under normal conditions

### Interpreting Results

**Stress Test Output:**
```
‚úÖ PASSED: Concurrent Subscribers
   Subscribers: 50/50, Messages per subscriber: 50, Time: 2.34s, Throughput: 1068 msg/s

‚úÖ PASSED: Backpressure Handling
   Published: 2000, Received: 856, Dropped: 1144, Publisher time: 1.23s, Total time: 45.67s
```

**Load Test Output:**
```
üìä LOAD TEST RESULTS
‚è±Ô∏è  Duration: 62.45s
üì® Messages: Published: 10,000, Received: 9,987, Dropped: 13 (0.13%)
üöÄ Throughput: Published: 160 msg/s, Received: 159 msg/s
‚ö° Latency: Average: 12.34ms, P50: 8.21ms, P95: 45.67ms, P99: 78.90ms, Max: 123.45ms
üéØ Overall Performance: ‚úÖ EXCELLENT - System handled load well
```

## üìö Further Reading

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [WebSocket Protocol](https://tools.ietf.org/html/rfc6455)
- [Python asyncio](https://docs.python.org/3/library/asyncio.html)

## üìÑ License

This is a technical assignment implementation. Use as needed.

---

**Built with ‚ù§Ô∏è using FastAPI and Python 3.11**
